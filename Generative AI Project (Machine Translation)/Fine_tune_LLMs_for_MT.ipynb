{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune a Generative AI Model for Machine Translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will fine-tune an existing LLM from Hugging Face for enhanced machine translation. I will use the [MarianMT](https://huggingface.co/docs/transformers/en/model_doc/marian) model, which provides a high quality instruction tuned model and can translation text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with BLEU metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ 1 - Load Required Dependencies, Dataset and LLM](#1)\n",
    "  - [ 1.1 - Set up Required Dependencies](#1.1)\n",
    "  - [ 1.2 - Load Dataset and LLM](#1.2)\n",
    "  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n",
    "- [ 2 - Perform Full Fine-Tuning](#2)\n",
    "  - [ 2.1 - Preprocess the Machine Translation Dataset](#2.1)\n",
    "  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n",
    "  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n",
    "  - [ 2.4 - Evaluate the Model Quantitatively (with BLEU Metric)](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Load Required Dependencies, Dataset and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 - Set up Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required packages for the LLM and datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.17.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (18.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.27.0)\n",
      "Requirement already satisfied: packaging in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from pandas->datasets==2.17.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from pandas->datasets==2.17.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from pandas->datasets==2.17.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 0.3.0a0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchdata==0.5.1 (from versions: 0.3.0a1, 0.3.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.8.0, 0.9.0, 0.10.0, 0.10.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchdata==0.5.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Load Dataset and LLM\n",
    "\n",
    "You are going to continue experimenting with the [En-Az](https://huggingface.co/datasets/Zarifa/English-To-Azerbaijani) Hugging Face dataset. It contains 5,000+ sentences with the corresponding manually labeled translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 5161\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"Zarifa/English-To-Azerbaijani\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained [MarianMT](https://huggingface.co/Helsinki-NLP/opus-mt-az-en) and its tokenizer directly from HuggingFace. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-az-en\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 56061952\n",
      "all model parameters: 56586240\n",
      "percentage of trainable model parameters: 99.07%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 - Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Test the model with the zero shot inferencing. You can see that the model struggles to translate the test compared to the baseline result, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Translate the following sentence.\n",
      "\n",
      "Başçı dörd illiyinə seçildi.\n",
      "\n",
      "Translation:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN Translation:\n",
      "The president was elected for four years.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Technology was elected for four years:\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "sentence = dataset['train'][index]['translation']['aze']\n",
    "translate = dataset['train'][index]['translation']['en']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate the following sentence.\n",
    "\n",
    "{sentence}\n",
    "\n",
    "Translation:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN Translation:\\n{translate}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Preprocess the Machine Translation Dataset\n",
    "\n",
    "You need to convert the sentence-translate (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Translate the following conversation` and to the start of the Translate with `Translate` as follows:\n",
    "\n",
    "Training prompt (Translation):\n",
    "```\n",
    "Translate the following conversation.\n",
    "\n",
    "    Başçı dörd illiyinə seçildi.\n",
    "    \n",
    "Translate: \n",
    "```\n",
    "\n",
    "Training response (Translate):\n",
    "```\n",
    "The president was elected for four years.\n",
    "```\n",
    "\n",
    "Then preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 5161\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Translate the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nTranslate: '\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in [ex['aze'] for ex in example['translation']]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer([ex['en'] for ex in example['translation']], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save some time in the lab, I will subsample the dataset:\n",
    "\n",
    "*Note* : I do not have validation so I will take other portion as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_training = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "tokenized_datasets_validation = tokenized_datasets.filter(lambda example, index: index % 1001 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of all three parts of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (52, 3)\n",
      "Validation: (6, 3)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation', 'input_ids', 'labels'],\n",
      "        num_rows: 5161\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets_training['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets_validation['train'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataset is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
    "\n",
    "Now utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./machine-translation-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=115,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    max_steps=10,\n",
    "    save_strategy=\"steps\",  # Save checkpoints during training\n",
    "    save_steps=5,           # Save every 5 steps\n",
    "    save_total_limit=2      # Keep only the 2 most recent checkpoints\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_training['train'],\n",
    "    eval_dataset=tokenized_datasets_validation['train'] # I pass the same because my dataset doesnt have validation and test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5615, 'learning_rate': 0.0, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5.9995, 'train_samples_per_second': 13.334, 'train_steps_per_second': 1.667, 'train_loss': 1.561526107788086, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.561526107788086, metrics={'train_runtime': 5.9995, 'train_samples_per_second': 13.334, 'train_steps_per_second': 1.667, 'train_loss': 1.561526107788086, 'epoch': 1.43})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./machine-translation-training-1736454923/checkpoint-5\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "As with many GenAI applications, a qualitative approach where I ask myself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), we can see how the fine-tuned model is able to create a reasonable translation of the sentence compared to the original inability to understand what is being asked of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN TRANSLATION:\n",
      "The president was elected for four years.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Thorn's sterling, the chief of us, was elected for four years:\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Thorn's sterling, the chief of us, was elected for four years:\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "sentence = dataset['train'][index]['translation']['aze']\n",
    "human_baseline_translate = dataset['train'][index]['translation']['en']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate the following sentence.\n",
    "\n",
    "{sentence}\n",
    "\n",
    "Translation:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN TRANSLATION:\\n{human_baseline_translate}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - Evaluate the Model Quantitatively (with BLEU Metric)\n",
    "\n",
    "The [BLEU metric](https://en.wikipedia.org/wiki/BLEU) helps quantify the validity of translation produced by models. It compares translation to a \"baseline\" translation which is usually created by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 19.2MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 9.64MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 18.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the outputs for the sample of the test dataset (only 10 sentences and translations to save time), and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "human_baseline_translates = []\n",
    "part_of_dataset = dataset['train'][0:10]['translation']\n",
    "\n",
    "for sentence in part_of_dataset:\n",
    "    sentences.append(sentence['aze'])\n",
    "    human_baseline_translates.append(sentence['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_translates</th>\n",
       "      <th>original_model_translates</th>\n",
       "      <th>instruct_model_translates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning, ladies and gentlemen!</td>\n",
       "      <td>Sufinith Wallen Wallenz, the tomorrow smoldering:</td>\n",
       "      <td>Sufiniths of Sufts, the tomorrow laity, is goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I give you my word.</td>\n",
       "      <td>Sufinzler, I give you a word:</td>\n",
       "      <td>Surezler, I give you a word, a saying:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good morning.</td>\n",
       "      <td>Sufinzler: Succeedingly, we're all in the same...</td>\n",
       "      <td>Sufinzler: Succeeding, smelting!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which is new?</td>\n",
       "      <td>Sufiniths: there's no, no water, no, no, no.</td>\n",
       "      <td>Sufiniths, there's no, no water, no, no, no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>Sufin Schrogener: Succeeding.</td>\n",
       "      <td>Succeeding at Succeeding:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Currently Burj Khalifa is the tallest skyscrap...</td>\n",
       "      <td>Smokesion - boggling chills: low-power in the ...</td>\n",
       "      <td>Smoothing, the Big Braille convict, is the hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Goodbye!</td>\n",
       "      <td>Sufin Schrootzler: No.m.</td>\n",
       "      <td>Sufinz, a superstitious constituent: Smooth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How are you getting on?</td>\n",
       "      <td>Sufinithing: how are you?</td>\n",
       "      <td>Sufinz, sterezing at the bottom of the smelts,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where do you come from?</td>\n",
       "      <td>Sufinith stereters: all of you at that point?</td>\n",
       "      <td>Sufini stereters: all of you at that time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Where is my newspaper?</td>\n",
       "      <td>Sufin Schroeder: Where is Succeeding?</td>\n",
       "      <td>Sufini is a sterling ion: where is my Succeory?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           human_baseline_translates  \\\n",
       "0                Good morning, ladies and gentlemen!   \n",
       "1                                I give you my word.   \n",
       "2                                      Good morning.   \n",
       "3                                      Which is new?   \n",
       "4                                              Yeah.   \n",
       "5  Currently Burj Khalifa is the tallest skyscrap...   \n",
       "6                                           Goodbye!   \n",
       "7                            How are you getting on?   \n",
       "8                            Where do you come from?   \n",
       "9                             Where is my newspaper?   \n",
       "\n",
       "                           original_model_translates  \\\n",
       "0  Sufinith Wallen Wallenz, the tomorrow smoldering:   \n",
       "1                      Sufinzler, I give you a word:   \n",
       "2  Sufinzler: Succeedingly, we're all in the same...   \n",
       "3       Sufiniths: there's no, no water, no, no, no.   \n",
       "4                      Sufin Schrogener: Succeeding.   \n",
       "5  Smokesion - boggling chills: low-power in the ...   \n",
       "6                           Sufin Schrootzler: No.m.   \n",
       "7                          Sufinithing: how are you?   \n",
       "8      Sufinith stereters: all of you at that point?   \n",
       "9              Sufin Schroeder: Where is Succeeding?   \n",
       "\n",
       "                           instruct_model_translates  \n",
       "0  Sufiniths of Sufts, the tomorrow laity, is goo...  \n",
       "1             Surezler, I give you a word, a saying:  \n",
       "2                   Sufinzler: Succeeding, smelting!  \n",
       "3       Sufiniths, there's no, no water, no, no, no.  \n",
       "4                          Succeeding at Succeeding:  \n",
       "5  Smoothing, the Big Braille convict, is the hig...  \n",
       "6       Sufinz, a superstitious constituent: Smooth.  \n",
       "7  Sufinz, sterezing at the bottom of the smelts,...  \n",
       "8         Sufini stereters: all of you at that time?  \n",
       "9    Sufini is a sterling ion: where is my Succeory?  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_translates = []\n",
    "instruct_model_translates = []\n",
    "peft_model_translates = []\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sentence}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_translates.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_translates.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_translates = list(zip(human_baseline_translates, original_model_translates, instruct_model_translates))\n",
    " \n",
    "df = pd.DataFrame(zipped_translates, columns = ['human_baseline_translates', 'original_model_translates', 'instruct_model_translates'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models computing BLEU metrics. Notice the improvement in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BLEU:\n",
      "{'bleu': 0.0, 'precisions': [0.20833333333333334, 0.08139534883720931, 0.02631578947368421, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.8461538461538463, 'translation_length': 96, 'reference_length': 52}\n",
      "INSTRUCT MODEL BLEU:\n",
      "{'bleu': 0.0, 'precisions': [0.17757009345794392, 0.07216494845360824, 0.022988505747126436, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.0576923076923075, 'translation_length': 107, 'reference_length': 52}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU for the original model\n",
    "original_model_results = bleu.compute(\n",
    "    predictions=original_model_translates,\n",
    "    references=human_baseline_translates[0:len(original_model_translates)]\n",
    ")\n",
    "\n",
    "# Compute BLEU for the instruct model\n",
    "instruct_model_results = bleu.compute(\n",
    "    predictions=instruct_model_translates,\n",
    "    references=human_baseline_translates[0:len(instruct_model_translates)]\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print('ORIGINAL MODEL BLEU:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL BLEU:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n",
    "\n",
    "PEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configure LoRA for the Helsinki model\n",
    "# Dynamically generate the target modules\n",
    "encoder_layers = [f\"model.encoder.layers.{i}.self_attn.{proj}\" for i in range(6) for proj in [\"k_proj\", \"v_proj\", \"q_proj\"]]\n",
    "decoder_self_attn_layers = [f\"model.decoder.layers.{i}.self_attn.{proj}\" for i in range(6) for proj in [\"k_proj\", \"v_proj\", \"q_proj\"]]\n",
    "decoder_cross_attn_layers = [f\"model.decoder.layers.{i}.encoder_attn.{proj}\" for i in range(6) for proj in [\"k_proj\", \"v_proj\", \"q_proj\"]]\n",
    "\n",
    "# Combine all target modules\n",
    "target_modules = encoder_layers + decoder_self_attn_layers + decoder_cross_attn_layers\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=target_modules,  # Explicitly specified target modules\n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    bias=\"none\",  # No bias reparameterization\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Sequence-to-sequence task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add LoRA adapter layers/parameters to the original LLM to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 884736\n",
      "all model parameters: 57470976\n",
      "percentage of trainable model parameters: 1.54%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Train PEFT Adapter\n",
    "\n",
    "Define training arguments and create `Trainer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-sentence-translate-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to train the PEFT adapter and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-machine-translation-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare this model by adding an adapter to the original MarianMT model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(original_model, \n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "Make inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "\n",
    "sentence = dataset['train'][index]['translation']['aze']\n",
    "human_baseline_translate = dataset['train'][index]['translation']['en']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate the following sentence.\n",
    "\n",
    "{sentence}\n",
    "\n",
    "Translation:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_translate}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Evaluate the Model Quantitatively (with BLEU Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 sentences and translations to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "human_baseline_translates = []\n",
    "part_of_dataset = dataset['train'][0:10]['translation']\n",
    "\n",
    "for sentence in part_of_dataset:\n",
    "    sentences.append(sentence['aze'])\n",
    "    human_baseline_translates.append(sentence['en'])\n",
    "\n",
    "original_model_translates = []\n",
    "instruct_model_translates = []\n",
    "peft_model_translates = []\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sentence}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = human_baseline_translates[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_translates.append(original_model_text_output)\n",
    "    instruct_model_translates.append(instruct_model_text_output)\n",
    "    peft_model_translates.append(peft_model_text_output)\n",
    "\n",
    "zipped_translates = list(zip(human_baseline_translates, original_model_translates, instruct_model_translates, peft_model_translates))\n",
    " \n",
    "df = pd.DataFrame(zipped_translates, columns = [\n",
    "    'human_baseline_translates', \n",
    "    'original_model_translates', \n",
    "    'instruct_model_translates', \n",
    "    'peft_model_translates']\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute BLEU score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU for the original model\n",
    "original_model_results = bleu.compute(\n",
    "    predictions=original_model_translates,\n",
    "    references=human_baseline_translates[0:len(original_model_translates)]\n",
    ")\n",
    "\n",
    "# Compute BLEU for the instruct model\n",
    "instruct_model_results = bleu.compute(\n",
    "    predictions=instruct_model_translates,\n",
    "    references=human_baseline_translates[0:len(instruct_model_translates)]\n",
    ")\n",
    "# Compute BLEU for the PEFT model\n",
    "instruct_model_results = bleu.compute(\n",
    "    predictions=peft_model_translates,\n",
    "    references=human_baseline_translates[0:len(peft_model_translates)]\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print('ORIGINAL MODEL BLEU:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL BLEU:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
