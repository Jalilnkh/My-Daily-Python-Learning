{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Use Case: Machine Translation\n",
    "\n",
    "Welcome to the practical side of my generative AI project. In this lab I will do the machine translation task using generative AI. I will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task I need. By comparing zero shot, one shot, and few shot inferences, I will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ 1 - Set up Required Dependencies](#1)\n",
    "- [ 2 - Machine translation without Prompt Engineering](#2)\n",
    "- [ 3 - Machine translation with an Instruction Prompt](#3)\n",
    "  - [ 3.1 - Zero Shot Inference with an Instruction Prompt](#3.1)\n",
    "  - [ 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5](#3.2)\n",
    "- [ 4 - Machine translation with One Shot and Few Shot Inference](#4)\n",
    "  - [ 4.1 - One Shot Inference](#4.1)\n",
    "  - [ 4.2 - Few Shot Inference](#4.2)\n",
    "- [ 5 - Generative Configuration Parameters for Inference](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Set up Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.17.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (18.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (0.27.0)\n",
      "Requirement already satisfied: packaging in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from datasets==2.17.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from aiohttp->datasets==2.17.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from pandas->datasets==2.17.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from pandas->datasets==2.17.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from pandas->datasets==2.17.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 0.3.0a0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchdata==0.5.1 (from versions: 0.3.0a1, 0.3.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.8.0, 0.9.0, 0.10.0, 0.10.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchdata==0.5.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, Large Language Model (LLM), tokenizer, and configurator. Do not worry if you do not understand yet all of those components - they will be described and discussed later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Machine translation without Prompt Engineering\n",
    "\n",
    "In this use case, you will be generating a translation between azerbaijani and english with the pre-trained Large Language Model (LLM) MarianMT from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index). \n",
    "\n",
    "Let's upload some simple translation from the [DialogSum](https://huggingface.co/datasets/Zarifa/English-To-Azerbaijani) Hugging Face dataset. This dataset contains 5,000+ sentence with the corresponding manually labeled translated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"Zarifa/English-To-Azerbaijani\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a couple of sentences with their baseline translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "It is time to go to school.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Məktəbə getmə vaxtı.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "The president was elected for four years.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Başçı dörd illiyinə seçildi.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['train'][index]['translation']['en'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['train'][index]['translation']['aze'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [MarianMT](https://huggingface.co/Helsinki-NLP/opus-mt-az-en), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-az-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform encoding and decoding, you need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models. \n",
    "\n",
    "Download the tokenizer for the MarianMT model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniaz/Desktop/jobs/2025/My-Daily-Python-Learning/venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-az-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the tokenizer encoding and decoding a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([3119, 1294, 2509,  560, 5190,  871, 8239,    5,    0])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "▁Başçı▁dörd▁illiyinə▁seçildi.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(dataset['train'][index]['translation']['aze'], return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM translate a test without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Məktəbə getmə vaxtı.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "It is time to go to school.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Stay up at school.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Başçı dörd illiyinə seçildi.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "The president was elected for four years.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "The chief was elected for four years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['train'][index]['translation']['aze']\n",
    "    summary = dataset['train'][index]['translation']['en']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
